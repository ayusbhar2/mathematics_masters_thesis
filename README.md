# mathematics_masters_thesis
My masters thesis uses an Algebraic Geometric approach to explore the critical points of the loss function of linear neural networks in $\mathbb{C}^N$.

In this thesis, we explore the critical points of the loss function of deep linear networks from an algebraic geometric perspective by viewing these critical points as solutions to a polynomial system associated with the network. We focus on 1-hidden layer networks, for which we identify a new upper bound, $\mathcal{B}_{\mathbb{C}^*}$, on the number of critical points that lie in $(\mathbb{C}^*)^N$, where $N$ is the number of weights in the network. We show that this upper bound is smaller (in some cases, orders of magnitude smaller) than the BKK bound on the polynomial system associated with the network. We also identify an upper bound, $\mathcal{B}_{\mathbb{C}}$, on the total number of complex critical points of the network.

Further, we explore the structure within the critical points of linear networks. In particular, for 1-hidden layer networks, we prove that critical points that lie outside $(\mathbb{C}^*)^N$ must lie on particular coordinate subspaces. For larger networks, we demonstrate this through experiments.
